{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO60gRqtntEdBnjQuvMZHbD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afranaima/Fungsi-Non-Linier-Menggunakan-metode-Gradient-Descent-dan-Newton/blob/main/Tugas_Komnum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import numpy as np # Remove extra space and separate import statements\n",
        "import time         # Remove extra space and put on a new line\n",
        "def grad_descent(alpha, x0, y0, tol, max_iter):\n",
        "    x, y = x0, y0  # Corrected indentation\n",
        "    for _ in range(max_iter):  # Corrected indentation and loop variable\n",
        "        grad_x, grad_y = 2 * x - 2, 2 * y - 4  # Corrected indentation\n",
        "        x_new, y_new = x - alpha * grad_x, y - alpha * grad_y  # Corrected indentation\n",
        "        if abs(f(x_new, y_new) - f(x, y)) < tol:  # Corrected indentation\n",
        "            break  # Corrected indentation\n",
        "        x, y = x_new, y_new  # Corrected indentation\n",
        "    return x, y  # Corrected indentation\n",
        "def newton_method(x0, y0, tol, max_iter):\n",
        "    x, y = x0, y0  # Corrected indentation\n",
        "    for _ in range(max_iter):  # Corrected indentation and loop variable\n",
        "        grad = np.array([2 * x - 2, 2 * y - 4])  # Corrected indentation\n",
        "        hessian_inv = np.linalg.inv(np.array([[2, 0], [0, 2]]))  # Corrected indentation\n",
        "        step = np.dot(hessian_inv, grad)  # Corrected indentation\n",
        "        x_new, y_new = x - step[0], y - step[1]  # Corrected indentation\n",
        "        if abs(f(x_new, y_new) - f(x, y)) < tol:  # Corrected indentation\n",
        "            break  # Corrected indentation\n",
        "        x, y = x_new, y_new  # Corrected indentation\n",
        "    return x, y  # Corrected indentation\n",
        "def f(x, y):\n",
        "    return x**2 + y**2 - 2 * x - 4 * y + 5  # Corrected indentation\n",
        "# Example usage\n",
        "alpha = 0.1\n",
        "x0, y0 = 0, 0\n",
        "tol = 1e-5\n",
        "max_iter = 100\n",
        "# Timing Gradient Descent\n",
        "start_time_gd = time.time()\n",
        "x_gd, y_gd = grad_descent(alpha, x0, y0, tol, max_iter)\n",
        "end_time_gd = time.time()\n",
        "execution_time_gd = end_time_gd - start_time_gd\n",
        "# Timing Newton Method\n",
        "start_time_nm = time.time()\n",
        "x_nm, y_nm = newton_method(x0, y0, tol, max_iter)\n",
        "end_time_nm = time.time()\n",
        "execution_time_nm = end_time_nm - start_time_nm\n",
        "print(f\"Gradient Descent: x = {x_gd}, y = {y_gd}, f(x, y) = {f(x_gd, y_gd)}, Execution time: {execution_time_gd} seconds\")\n",
        "print(f\"Newton Method: x = {x_nm}, y = {y_nm}, f(x, y) = {f(x_nm, y_nm)}, Execution time: {execution_time_nm} seconds\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe4ghBSDWniT",
        "outputId": "a1930297-8153-420a-8fb1-9644a552e52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Descent: x = 0.9980657186886166, y = 1.9961314373772332, f(x, y) = 1.8707220958091852e-05, Execution time: 0.0001456737518310547 seconds\n",
            "Newton Method: x = 1.0, y = 2.0, f(x, y) = 0.0, Execution time: 0.00479435920715332 seconds\n"
          ]
        }
      ]
    }
  ]
}